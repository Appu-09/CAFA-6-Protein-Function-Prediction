# -*- coding: utf-8 -*-
"""CAFA 6 Protein Function Prediction _Aparna Vemuganti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GFaNC6nLKK5TORTSyk28OwOK28h2yfo5

CAFA-6 Protein Function Prediction Assignment

1ï¸âƒ£ Introduction / Objective

The Critical Assessment of Functional Annotation (CAFA) challenge aims to evaluate computational methods for predicting the biological function of proteins.
In this project, we focus on CAFA-6, where the goal is to predict Gene Ontology (GO) terms for given protein sequences.

We use provided datasets containing:

- Protein sequences (amino acid chains)

- Their associated GO terms (functional labels)

- Ontology categories (BP, MF, CC)

Our objective is to build machine learning models that can automatically predict protein functions based on their sequences.
"""

!pip install biopython obonet tqdm scikit-learn matplotlib

"""We install the required libraries:

- biopython â€” for reading FASTA sequences

- obonet â€” for parsing Gene Ontology (OBO files)

- tqdm â€” for progress bars

- scikit-learn â€” for ML models and preprocessing

- matplotlib â€” for data visualization
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c cafa-6-protein-function-prediction

"""Moves your Kaggle API key to the correct location and sets file permissions to be secure."""

!unzip -q cafa-6-protein-function-prediction.zip

!ls -lh

!ls -lh Train
!ls -lh Test

import pandas as pd
from Bio import SeqIO

# Paths
train_seq_path = "Train/train_sequences.fasta"
train_terms_path = "Train/train_terms.tsv"
train_taxonomy_path = "Train/train_taxonomy.tsv"
go_path = "Train/go-basic.obo"

# Load TSVs
train_terms = pd.read_csv(train_terms_path, sep="\t", header=None, names=["Protein_ID", "GO_ID", "Ontology"])
train_tax = pd.read_csv(train_taxonomy_path, sep="\t")

print("Train terms shape:", train_terms.shape)
print("Train taxonomy shape:", train_tax.shape)

# Load FASTA
train_sequences = []
for record in SeqIO.parse(train_seq_path, "fasta"):
    train_sequences.append({"Protein_ID": record.id, "Sequence": str(record.seq)})

train_sequences = pd.DataFrame(train_sequences)
print("Train sequences shape:", train_sequences.shape)

# Peek
train_sequences.head()

"""Loads all standard data analysis libraries used for reading, visualizing, and manipulating the data.
- Defines the path to each dataset file for easy access later in the notebook.
- Reads the GO-term annotations and taxonomy data into Pandas DataFrames.

train_terms: maps each protein to one or more GO terms.

train_tax: links each protein to its species (taxon ID).
- Parses the FASTA sequence file and creates a DataFrame of all proteins,
storing both the protein ID and the amino-acid sequence.
- Adds a column with the sequence length of each protein.
Checking this helps understand the distribution and range of protein sizes.

"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1ï¸âƒ£ Sequence length distribution
train_sequences["Seq_Len"] = train_sequences["Sequence"].apply(len)
plt.figure(figsize=(8,4))
sns.histplot(train_sequences["Seq_Len"], bins=50, kde=True)
plt.title("Protein Sequence Length Distribution")
plt.xlabel("Sequence length")
plt.ylabel("Count")
plt.show()

# 2ï¸âƒ£ GO terms per protein
terms_per_protein = train_terms.groupby("Protein_ID")["GO_ID"].count()
plt.figure(figsize=(8,4))
sns.histplot(terms_per_protein, bins=40, kde=False)
plt.title("Number of GO Terms per Protein")
plt.xlabel("# of GO Terms")
plt.ylabel("Count")
plt.show()

# 3ï¸âƒ£ Most frequent GO terms
top_terms = train_terms["GO_ID"].value_counts().head(15)
plt.figure(figsize=(10,4))
sns.barplot(x=top_terms.index, y=top_terms.values)
plt.xticks(rotation=45, ha="right")
plt.title("Top 15 Most Common GO Terms")
plt.ylabel("Frequency")
plt.show()

# 4ï¸âƒ£ Ontology type counts
plt.figure(figsize=(5,4))
sns.countplot(x="Ontology", data=train_terms)
plt.title("Counts by Ontology Type (MF / BP / CC)")
plt.show()

"""Plots how protein sequence lengths are distributed.

This gives insights into variabilityâ€”some models might need to truncate or pad sequences.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer

# --- Step 1: Check ID formats ---
print("Sample IDs from train_terms:")
print(train_terms["Protein_ID"].head(10).tolist())

print("\nSample IDs from train_sequences:")
print(train_sequences["Protein_ID"].head(10).tolist())

# --- Step 2: Normalize Protein_ID formats ---
# (strip off "sp|" or other prefixes/suffixes to make them match)
train_sequences["Protein_ID_clean"] = train_sequences["Protein_ID"].str.extract(r'([A-Z0-9]+)')  # captures UniProt IDs like A0A0C5B5G6
train_terms["Protein_ID_clean"] = train_terms["Protein_ID"].str.extract(r'([A-Z0-9]+)')

# --- Step 3: Merge on the cleaned IDs ---
merged = (
    train_terms.groupby("Protein_ID_clean")["GO_ID"].apply(list).reset_index()
    .merge(train_sequences[["Protein_ID_clean", "Sequence", "Seq_Len"]], on="Protein_ID_clean")
)

print("\nMerged shape:", merged.shape)
print(merged.head())

# --- Step 4: Filter top N GO terms ---
N = 200
top_terms = train_terms["GO_ID"].value_counts().head(N).index
merged["GO_ID"] = merged["GO_ID"].apply(lambda gos: [g for g in gos if g in top_terms])
merged = merged[merged["GO_ID"].map(len) > 0]

print("Filtered shape:", merged.shape)

# --- Step 5: Train/Validation Split ---
if len(merged) > 0:
    train_df, val_df = train_test_split(merged, test_size=0.2, random_state=42)
    print("Train/Val shapes:", train_df.shape, val_df.shape)
else:
    print("âš ï¸ Still empty after filtering â€” check ID normalization above.")

"""Protein IDs in the datasets had different formats.

This step normalizes them, merges the GO term annotations with their corresponding protein sequences,
and groups multiple GO terms per protein into a list.

Selects the top 200 most frequent GO terms to make the classification tractable.
Proteins without these GO terms are filtered out.
Then the data is split into training and validation sets.
"""

# --- Step 5: Encode GO terms for multi-label classification ---
mlb = MultiLabelBinarizer()
y_train = mlb.fit_transform(train_df["GO_ID"])
y_val = mlb.transform(val_df["GO_ID"])

print("Label matrix shapes:")
print("Train labels:", y_train.shape)
print("Val labels:", y_val.shape)

# Check top 5 GO terms encoded
print("\nTop GO terms:", mlb.classes_[:10])

"""Converts lists of GO terms per protein into a multi-label binary matrix where each column represents a GO term.

This enables multi-output classification.
"""

import numpy as np
from sklearn.preprocessing import StandardScaler

# --- Step 6: Simple amino acid frequency features ---
def extract_features(seq):
    amino_acids = "ACDEFGHIKLMNPQRSTVWY"
    seq = seq.upper()
    total = len(seq)
    freq = [seq.count(a) / total for a in amino_acids]
    return freq + [total]  # add sequence length as last feature

X_train = np.array([extract_features(s) for s in train_df["Sequence"]])
X_val = np.array([extract_features(s) for s in val_df["Sequence"]])

print("Feature matrix shapes:")
print("X_train:", X_train.shape)
print("X_val:", X_val.shape)

# --- Standardize features ---
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

"""This function converts each protein sequence (string of amino acids) into a simple numerical feature vector.
Letâ€™s break it down:

- amino_acids â†’ defines the 20 standard amino acids.

- seq.upper() â†’ ensures uniform case (proteins are case-insensitive).

- total = len(seq) â†’ finds the total sequence length.

- freq = [seq.count(a) / total for a in amino_acids] â†’
calculates how often each amino acid appears, divided by total length â†’ i.e., normalized frequency.

So each sequence becomes a 20-element list (one value per amino acid).

return freq + [total] â†’ adds the total sequence length as an extra feature.

âœ… Output: Each protein â†’ 21 numeric features (20 frequencies + 1 length).
"""

from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import f1_score, accuracy_score

# --- Step 7: Baseline Model Training ---
log_reg = OneVsRestClassifier(
    LogisticRegression(max_iter=200, solver='liblinear')
)

print("Training Logistic Regression model...")
log_reg.fit(X_train, y_train)
print("âœ… Training complete!")

# --- Step 8: Predictions and Evaluation ---
y_pred = log_reg.predict(X_val)

# Compute metrics
f1_micro = f1_score(y_val, y_pred, average="micro")
f1_macro = f1_score(y_val, y_pred, average="macro")

print(f"\nValidation Micro F1-score: {f1_micro:.4f}")
print(f"Validation Macro F1-score: {f1_macro:.4f}")

"""Trains a baseline One-Vs-Rest Logistic Regression model.

Each GO term is treated as an independent binary classification problem.

The F1-scores evaluate how well the model predicts multiple GO terms simultaneously.
"""

from sklearn.feature_extraction.text import CountVectorizer

# --- Step 9A: Generate k-mer "sentences" ---
def kmers(seq, k=3):
    seq = seq.upper()
    return " ".join([seq[i:i+k] for i in range(len(seq)-k+1)])

train_kmers = [kmers(s) for s in train_df["Sequence"]]
val_kmers = [kmers(s) for s in val_df["Sequence"]]

print("Example 3-mer representation:")
print(train_kmers[0][:120], "...")

"""Breaks each protein sequence into overlapping 3-mers (triplets of amino acids)
and uses a bag-of-words approach to convert them into numeric feature vectors.
These features capture local sequence patterns.
"""

# --- Step 9B: Convert k-mers into numerical vectors ---
vectorizer = CountVectorizer(max_features=3000)  # limit to top 3000 3-mers
X_train_kmer = vectorizer.fit_transform(train_kmers)
X_val_kmer = vectorizer.transform(val_kmers)

print("K-mer feature matrix shapes:")
print("X_train_kmer:", X_train_kmer.shape)
print("X_val_kmer:", X_val_kmer.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.metrics import f1_score

# --- Step: Use smaller subset for faster testing ---
subset_train = X_train_kmer[:5000]
subset_y = y_train[:5000]
subset_val = X_val_kmer[:1000]
subset_y_val = y_val[:1000]

rf_small = MultiOutputClassifier(
    RandomForestClassifier(
        n_estimators=10,     # smaller forest
        max_depth=10,        # limit complexity
        n_jobs=-1,
        random_state=42
    )
)

print("Training small Random Forest subset model âš¡")
rf_small.fit(subset_train, subset_y)
print("âœ… Training done!")

# Evaluate
y_pred_small = rf_small.predict(subset_val)
f1_micro_small = f1_score(subset_y_val, y_pred_small, average="micro")
f1_macro_small = f1_score(subset_y_val, y_pred_small, average="macro")

print(f"\nValidation Micro F1-score (small RF): {f1_micro_small:.4f}")
print(f"Validation Macro F1-score (small RF): {f1_macro_small:.4f}")

import pandas as pd
from Bio import SeqIO
import numpy as np

# --- Step 1: Load test sequences ---
test_fasta_path = "/content/Test/testsuperset.fasta"

test_records = list(SeqIO.parse(test_fasta_path, "fasta"))
test_df = pd.DataFrame({
    "Protein_ID": [r.id for r in test_records],
    "Sequence": [str(r.seq) for r in test_records]
})

print("Loaded test sequences:", len(test_df))

# --- Step 2: K-mer feature extraction (same as training) ---
def kmer_features(sequence, k=3):
    sequence = sequence.upper()
    kmers = [sequence[i:i+k] for i in range(len(sequence) - k + 1)]
    return " ".join(kmers)

test_kmers = [kmer_features(seq) for seq in test_df["Sequence"]]

# --- Step 3: Transform using the same TF-IDF vectorizer ---
X_test_kmer = vectorizer.transform(test_kmers)
print("X_test_kmer shape:", X_test_kmer.shape)

# --- Step 4: Predict probabilities using trained model ---
print("Predicting probabilities... this might take a few minutes â³")
y_pred_proba = rf_small.predict_proba(X_test_kmer)

# --- Step 5: Build submission rows ---
submission_rows = []
for i, prot_id in enumerate(test_df["Protein_ID"]):
    for j, go_id in enumerate(mlb.classes_):
        try:
            score = y_pred_proba[j][:, 1][i]
        except:
            score = 0.0
        if score > 0:
            submission_rows.append([prot_id, go_id, round(float(score), 3)])

submission_df = pd.DataFrame(submission_rows, columns=["Protein_ID", "GO_ID", "Score"])

# --- Step 6: Save file ---
submission_df.to_csv("submission.tsv", sep="\t", index=False)
print("âœ… submission.tsv file created with", len(submission_df), "rows.")

!ls -lh submission.tsv
!head submission.tsv

"""## ðŸ§© Final Submission File

The final predictions for the CAFA 6 Protein Function Prediction task were generated using the trained Random Forest model with TF-IDF k-mer features.

- **File name:** `submission.tsv`
- **Rows:** 43,190,202
- **Columns:** `Protein_ID`, `GO_ID`, `Score`
- **File size:** ~975 MB
- **Format:** Tab-separated values (`.tsv`), as required by Kaggle.

"""